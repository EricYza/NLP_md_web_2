---
title: "About Me" 
layout: page
multilingual: true
draft: false   
---

# About Me

> **NLP Engineer · Applied Researcher · LLM Alignment & Reasoning**
> Bridging research and production—designing, training, and deploying language models that reason, align, and communicate reliably.

---

## Executive Summary

| Dimension   | Highlights                                                                                       | Evidence                           |
| ----------- | ------------------------------------------------------------------------------------------------ | ---------------------------------- |
| Research    | LLM alignment (DPO/RLHF), reasoning chains, multilingual RAG, efficient fine-tuning (LoRA/QLoRA) | Papers & internal experiments      |
| Engineering | Distributed training/inference, evaluation pipeline, continuous deployment                       | System design & iteration cadence  |
| Impact      | Reusable frameworks, mentoring, open-source toolkits                                             | GitHub repos & technical articles  |
| Method      | “Evaluation-first, data-centric, and reproducibility-driven”                                     | Reliable model dev & release logs  |

---

## Core Value

* **From SOTA to SOP**: translate novel research into stable, reproducible training workflows.  
* **From Models to Systems**: deploy and monitor models that serve millions of users.  
* **From Metrics to Experience**: close the gap between offline metrics and real-world behavior.  
* **From Chaos to Clarity**: build clean abstractions and reliable pipelines in complex ML stacks.  

---

## Skill Matrix

| Area                     | Subskills                                                     | Proficiency | Notes                                     |
| ------------------------ | ------------------------------------------------------------- | ----------- | ----------------------------------------- |
| Training & Optimization  | PyTorch, DeepSpeed, FSDP, LoRA/QLoRA, mixed precision         | ★★★★★       | End-to-end LLM training & debugging       |
| Alignment & Preference   | SFT, DPO/IPO, RLHF/RLAIF, reward modeling                     | ★★★★★       | Stability, safety, and convergence focus  |
| Reasoning & Agents       | CoT/ToT/GoT, function calling, multi-tool planning            | ★★★★☆       | Modular reasoning & agentic behavior      |
| RAG                      | Chunking, reranking, hybrid search, citation grounding        | ★★★★☆       | Long-document & compliance QA systems     |
| Evaluation & Reliability | Hallucination tests, factuality, robustness, uncertainty eval | ★★★★★       | Automated regression & failure analysis   |
| Systems & Infra          | Distributed clusters, CI/CD, tracing, monitoring              | ★★★★☆       | Scalable and reproducible ML pipelines    |

---

## Research Interests

* **Alignment & Preference Learning** — optimizing human-model alignment via DPO and offline RL.  
* **Reasoning & Tool Use** — teaching models to read, plan, and act coherently.  
* **Efficiency & Adaptation** — low-rank finetuning, quantization, and cross-domain generalization.  
* **Evaluation & Trustworthiness** — measuring hallucination, bias, and interpretability in LLMs.  
* **Retrieval-Augmented Generation (RAG)** — scalable, source-grounded generation for enterprise QA.  

---

## Selected Projects

1. **Reasoning-Enhanced LLM Pipeline**  
   * Designed hierarchical reasoning prompts with adaptive self-correction.  
   * Improved task success rate by 18% and interpretability across reasoning benchmarks.  

2. **Lightweight DPO Alignment Framework**  
   * Built a reproducible RLHF alternative using DPO with preference bootstrapping.  
   * Matched RLHF-level alignment with 1/5 of the compute budget.  

3. **Faithful RAG for Legal & Financial QA**  
   * Implemented reranker-based citation grounding and hallucination detection.  
   * Deployed in enterprise compliance assistant with verifiable tracebacks.  

4. **Unified Evaluation Platform for LLM Regression Testing**  
   * Automated daily A/B comparisons across reasoning, safety, and factuality.  
   * Integrated observability metrics and alerting for model drift detection.  

---

## Playbooks (Methods)

* **Data Governance** — deduplication, detox, and reward data filtering.  
* **Stable Training** — gradient clipping, warmup restarts, adaptive loss scaling.  
* **Trustworthy Evaluation** — scenario-based testing and red-teaming.  
* **Safe Deployment** — staged rollout, guardrails, and rollback readiness.  

---

## Publications & Writing

| Type     | Authors               | Title                                                                                   | Venue/Journal            | Year | Notes                                       |
| -------- | -------------------- | --------------------------------------------------------------------------------------- | ------------------------ | ---- | ------------------------------------------- |
| Preprint | Eric Chen et al.     | **Direct Preference Optimization with Difficulty-Aware Sampling**                      | arXiv                    | 2025 | Scaling alignment efficiently               |
| Preprint | Eric Chen et al.     | **Retrieval-Grounded Reasoning Agents for Knowledge-Intensive Tasks**                  | arXiv                    | 2025 | Tool-based reasoning and citation grounding |
| Blog     | Eric Chen            | **Making DPO Reproducible: Lessons from RLHF-lite**                                    | Medium / Tech Blog        | 2024 | Engineering reproducible preference tuning  |

---

## Talks & Workshops

| Format              | Topic                                         | Audience/Context                 | Deliverables                       |
| ------------------- | --------------------------------------------- | -------------------------------- | ---------------------------------- |
| Internal Workshop   | From RLHF to DPO: Practical Tradeoffs         | Research & Applied ML Teams      | Code demos, metrics, postmortems   |
| Technical Tutorial  | Reproducible RAG for Enterprise QA            | Industry Partners                | Evaluation checklists, configs     |
| Meetup Talk         | Efficient Finetuning with LoRA and QLoRA      | Open-Source Community            | Slides & Colab notebooks           |

---

## Open Source & Community

* Maintains **ReAlign**, a lightweight DPO/RLHF pipeline for educational use.  
* Contributor to evaluation toolkits for factuality and long-context benchmarks.  
* Active mentor in open-source communities promoting reproducibility and model alignment ethics.  

---

## Education

* **M.S., Computer Science** — University of Illinois Urbana-Champaign  
  Focus: NLP, Reinforcement Learning, and Trustworthy AI.  
* **B.S., Computer Engineering** — Tsinghua University  
  Minor: Applied Mathematics.  

---

## Contact

* **Email**: Eric.chen [at] 163.com  
* **GitHub**: [github.com/Ericchen-ml](https://github.com/Ericchen-ml)  
* **Blog**: [Ericchen.dev](https://Ericchen.dev)  
* **LinkedIn**: [linkedin.com/in/Ericchen-ml](https://linkedin.com/in/Ericchen-ml)  

---

## TL;DR

I’m an **NLP engineer** focusing on **LLM alignment, reasoning, and reliability**, passionate about turning **state-of-the-art research into scalable systems**. I build models that think before they speak—and systems that learn from every output.
